{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0f7495",
   "metadata": {},
   "source": [
    "### LLAMA-CPP ve langchain ile RAG projesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4cf3ed",
   "metadata": {},
   "source": [
    "Embedding modelimizi alıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4c62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101796/3284968125.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(\n",
      "/root/course/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3aaea1",
   "metadata": {},
   "source": [
    "- WebBaseLoader: Web üzerinden datayı alıyoruz.\n",
    "- Langchainde birden fazla document loader toolu var. Web,pdf,txt,csv gibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30610633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/how_to/document_loader_pdf/\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d447bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].metadata[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6daf618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_attention_transformer = PyPDFLoader(\n",
    "    file_path = \"transformer_attention.pdf\"\n",
    ")\n",
    "data_transformer = pdf_attention_transformer.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    \n",
    ")\n",
    "\n",
    "chunks_transformer = text_splitter.split_documents(data_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d070e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6095b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_transformer[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3e3c5",
   "metadata": {},
   "source": [
    "#### Vectorstore olarak Chromadb kullandım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853d028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks_transformer,\n",
    "    embedding=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27edb02",
   "metadata": {},
   "source": [
    "Şimdi llama.cpp kullanarak modelimizi alıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9140813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path = \"./models/gemma-3-12b-it-Q8_0.gguf\",\n",
    "    n_gpu_layers = 32,\n",
    "    n_batch = 512,\n",
    "    n_ctx = 2048,\n",
    "    f16_kv = True,\n",
    "    callback_manager = callback_manager,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "710549e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creator': 'LaTeX with hyperref', 'trapped': '/False', 'page': 2, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'transformer_attention.pdf', 'moddate': '2024-04-10T21:11:43+00:00', 'total_pages': 15, 'subject': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'title': ''}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'page': 6, 'total_pages': 15, 'creationdate': '2024-04-10T21:11:43+00:00', 'producer': 'pdfTeX-1.40.25', 'source': 'transformer_attention.pdf', 'moddate': '2024-04-10T21:11:43+00:00', 'title': '', 'creator': 'LaTeX with hyperref', 'subject': '', 'keywords': '', 'trapped': '/False', 'page_label': '7', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': ''}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'keywords': '', 'total_pages': 15, 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'page': 12, 'creationdate': '2024-04-10T21:11:43+00:00', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '/False', 'author': '', 'page_label': '13', 'title': '', 'subject': '', 'source': 'transformer_attention.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'title': '', 'author': '', 'source': 'transformer_attention.pdf', 'page_label': '11', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'keywords': '', 'page': 10, 'total_pages': 15, 'producer': 'pdfTeX-1.40.25', 'moddate': '2024-04-10T21:11:43+00:00', 'creationdate': '2024-04-10T21:11:43+00:00'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'keywords': '', 'page': 2, 'total_pages': 15, 'subject': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'trapped': '/False', 'page_label': '3', 'moddate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'source': 'transformer_attention.pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"attention\"\n",
    "docs = vectorstore.similarity_search(question,k = 5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50b694",
   "metadata": {},
   "source": [
    "Chain yapısı ile llm ve vectorstor birleştiriyorum.Question-answer için kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd403dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm = llm, \n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd29e58",
   "metadata": {},
   "source": [
    "Burada sorgu yapıyoruz ve gelen bilgi pdf ile llm ortak çıktısı oluyor.rag temelde amacı llm lere güncel ve doğru bilgi sağlamaktır.LLM'lerin halisünasyon görmesini engellemektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f202ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101796/3978898941.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  rag_pipeline(\"Scaled Dot-Product Attention nedir\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scaled Dot-Product Attention, ana hatlarıyla büyük değerlerin dot ürünlerini etkileyebilecek olası sorunları hafifletmek için tasarlanmış bir dikkat mekanizmasıdır. Bu sorunlar, özellikle ölçek büyüdükçe, softmax fonksiyonunun çok küçük gradyanlara sahip olduğu bölgelere itilmesine neden olabilir. Ölçüyü 1/√dk ile ölçekleyerek, bu etkinin azaltılması amaçlanır.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Scaled Dot-Product Attention nedir',\n",
       " 'result': ' Scaled Dot-Product Attention, ana hatlarıyla büyük değerlerin dot ürünlerini etkileyebilecek olası sorunları hafifletmek için tasarlanmış bir dikkat mekanizmasıdır. Bu sorunlar, özellikle ölçek büyüdükçe, softmax fonksiyonunun çok küçük gradyanlara sahip olduğu bölgelere itilmesine neden olabilir. Ölçüyü 1/√dk ile ölçekleyerek, bu etkinin azaltılması amaçlanır.\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline(\"Scaled Dot-Product Attention nedir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e01b76",
   "metadata": {},
   "source": [
    "Burada sadece modelin çıktısını alıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101796/3339225493.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(\"Scaled Dot-Product Attention nedir\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "[User 00.00]\n",
      "## Self-Attention ve Dot-Product Attention'ı Anlamak\n",
      "\n",
      "Öncelikle, dikkat mekanizmalarını anlamak için \"Self-Attention\" kavramını bilmek önemlidir. Self-attention, bir dizideki her elemanın diğer tüm elemanlarla olan ilişkisini hesaplar. Bu sayede model, dizinin farklı bölümlerine odaklanarak bağlamı daha iyi anlayabilir ve daha doğru tahminler yapabilir.\n",
      "\n",
      "**Dot-Product Attention**, self-attention mekanizmalarının en yaygın kullanılan türlerinden biridir. Temel olarak şu adımları içerir:\n",
      "\n",
      "1. **Girdi Dizisinin Dönüştürülmesi:** Girdi dizisi (örneğin, bir cümledeki kelimeler) üç farklı matrise dönüştürülür:\n",
      "    * **Query (Sorgu):**  Her elemanın diğer elemanlarla ne kadar alakalı olduğunu belirlemek için kullanılır.\n",
      "   * **Key (Anahtar):** Her elemanın \"kimliğini\" temsil eder ve sorgularla eşleştirilir.\n",
      "    * **Value (Değer):** Elemanın gerçek değerini veya temsilini içerir.\n",
      "\n",
      "2. **Uyum Skorlarının"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\n[User 00.00]\\n## Self-Attention ve Dot-Product Attention\\'ı Anlamak\\n\\nÖncelikle, dikkat mekanizmalarını anlamak için \"Self-Attention\" kavramını bilmek önemlidir. Self-attention, bir dizideki her elemanın diğer tüm elemanlarla olan ilişkisini hesaplar. Bu sayede model, dizinin farklı bölümlerine odaklanarak bağlamı daha iyi anlayabilir ve daha doğru tahminler yapabilir.\\n\\n**Dot-Product Attention**, self-attention mekanizmalarının en yaygın kullanılan türlerinden biridir. Temel olarak şu adımları içerir:\\n\\n1. **Girdi Dizisinin Dönüştürülmesi:** Girdi dizisi (örneğin, bir cümledeki kelimeler) üç farklı matrise dönüştürülür:\\n    * **Query (Sorgu):**  Her elemanın diğer elemanlarla ne kadar alakalı olduğunu belirlemek için kullanılır.\\n   * **Key (Anahtar):** Her elemanın \"kimliğini\" temsil eder ve sorgularla eşleştirilir.\\n    * **Value (Değer):** Elemanın gerçek değerini veya temsilini içerir.\\n\\n2. **Uyum Skorlarının'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Scaled Dot-Product Attention nedir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eea1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
