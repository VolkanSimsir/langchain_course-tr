{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21562f31",
   "metadata": {},
   "source": [
    "### LLama-cpp-python ile kullanımı için Türkçe kaynak\n",
    "\n",
    "llama.cpp, Meta'nın LLaMA ve diğer desteklediği modellerini CPU ve düşük kaynaklı donanımlarda çalıştırmak amacıyla yazılmış, C/C++ tabanlı, oldukça hızlı ve hafif bir inference (çıkarım) yapmakta.\n",
    "\n",
    "Aşağıda hem llama.cpp'in hem de llama-cpp-python'ın ne olduğu, nasıl kullanıldığı ve hangi özellikleri desteklediği üzerine detaylı bir anlatım yapacağım.\n",
    "\n",
    "- LLaMA ve diğer desteklediği modellerini yerel olarak çalıştırabilir.\n",
    "- CUDA, Metal (Apple), OpenBLAS, AVX2 gibi hızlandırmalarla performanslı inference yapabilir,\n",
    "- API server olarak çalıştırıp OpenAI API uyumlu hale getirebilir,\n",
    "- AVX, AVX2, AVX512, NEON gibi SIMD optimizasyonları\n",
    "- GGUF formatındaki modelleri destekler \n",
    "- Yerel Çalıştırma: Bulut bağımlılığı olmadan kendi donanımınızda çalıştırma\n",
    "- Hafifletilmiş Modeller: GGUF formatıyla quantize edilmiş (küçültülmüş) modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76421e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_cpp\n",
    "llama_cpp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e79f024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mu kullanılıyor: True\n",
      "GPU Adı: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU mu kullanılıyor: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Adı: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb3ff8",
   "metadata": {},
   "source": [
    "### GGUF Model Dosyasını İndirmek\n",
    "\n",
    "Bunun için birkaç yöntem göstereceğim.\n",
    "\n",
    "1) HF üzerinden direkt locale indrimek. Donanım durumunuza göre İstediğiniz quantize modeli indirebilirsiniz.\n",
    "2) Hf ile aşağıdaki kod ile indirebilirsiniz.\n",
    "\n",
    "Quantize seviyesi (Q4_K_M, Q5_K_S vb.) model kalitesi ve boyutu arasında trade-off sağlar.\n",
    "Bazı modeller (özellikle orijinal Meta modelleri) için erişim izni gerekebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"unsloth/gemma-3-12b-it-GGUF\"\n",
    "model_file = \"gemma-3-12b-it-Q8_0.gguf\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name,\n",
    "    filename=model_file,\n",
    "    local_dir=\"./models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa274ce",
   "metadata": {},
   "source": [
    "### Şimdi llama_cpp kullanabilriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ca7d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path= \"./models/gemma-3-12b-it-Q8_0.gguf\", #locale indirdiğimiz modeli alıyoruz.\n",
    "    n_gpu_layers=-1,\n",
    "    seed = 1337,\n",
    "    n_ctx=2048,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8108cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(\"Q: Selam A: \",\n",
    "           max_tokens=32,\n",
    "           stop=[\"Q:\", \"\\n\"],\n",
    "            echo = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c7d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-fdd08b36-9414-462e-84a3-94ad889372a3',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1745514201,\n",
       " 'model': './models/llama-2-7b.Q4_K_M.gguf',\n",
       " 'choices': [{'text': 'Q: Selam A: 01. nobody can tell me why the following code does not work?',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 16, 'total_tokens': 24}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70fd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: Selam A: 01. nobody can tell me why the following code does not work?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb8c31",
   "metadata": {},
   "source": [
    "### HF'den pulling model çekme işlemi\n",
    "Llama.from_pretrained ile modeli çekiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm_12b = Llama.from_pretrained(\n",
    "    repo_id = \"unsloth/gemma-3-12b-it-GGUF\",\n",
    "    filename=\"gemma-3-12b-it-Q4_K_M.gguf\",\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "938d57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm_12b(\"Q: Selam nasılsın A: \",\n",
    "           max_tokens=32,\n",
    "           stop=[\"Q:\", \"\\n\"],\n",
    "            echo = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6dfbfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: Selam nasılsın A: '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0cd1b",
   "metadata": {},
   "source": [
    "### Llama.cpp'de Chat Completion API Kullanımı\n",
    "\n",
    "Llama.cpp'nin yüksek seviye Python API'si, sohbet tamamlama (chat completion) için basit bir arayüz sunar. Bu özellik, modelin birden fazla mesajı tek bir prompt'a nasıl formatlayacağını bilmesini gerektirir.\n",
    "\n",
    "Llama sınıfı, mesajları tek bir prompt'a dönüştürürken şu sırayı izler:\n",
    "\n",
    "1) chat_handler parametresi verilmişse bunu kullanır\n",
    "\n",
    "2) chat_format parametresi verilmişse bunu kullanır\n",
    "\n",
    "3) GGUF modelinin metadata'sındaki tokenizer.chat_template'i kullanır.\n",
    "4) Hiçbiri yoksa llama-2 chat formatını kullanır\n",
    "5) Çoklu-turn diyalog desteği sağlar\n",
    "\n",
    "6) Sistem talimatlarını kullanıcı mesajlarından ayırır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ecf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./models/gemma-3-12b-it-Q8_0.gguf\", #model_path: GGUF formatındaki modelin dosya yolu\n",
    "    chat_format=\"gemma\" #chat_format: Kullanılacak sohbet formatı (örneğin \"llama-2\", \"chatml\", \"gemma\" vb.)\n",
    ")\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[ #messages: Sohbet mesajlarının listesi. \n",
    "        {\"role\":\"system\",\"content\":\"You are a perfect assistant\"},\n",
    "        {\"role\":\"user\",\"content\":\"Nasılsın ?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dfba2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ben bir dil modeliyim, bu yüzden duygularım yok. Ancak, bana sorduğun için teşekkür ederim! Şu anda iyiyim, senin nasıl olduğunu merak ediyorum?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0][\"message\"]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18269558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 8 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   15802.51 ms\n",
      "llama_perf_context_print: prompt eval time =   27227.16 ms /    16 tokens ( 1701.70 ms per token,     0.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10695.28 ms /    12 runs   (  891.27 ms per token,     1.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   17785.46 ms /    28 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Merhaba! Nasılsınız? Size nasıl yardımcı olabilirim?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages=[ #messages: Sohbet mesajlarının listesi. \n",
    "        {\"role\":\"system\",\"content\":\"Harika bir asistansın\"},\n",
    "        {\"role\":\"user\",\"content\":\"merhaba?\"}\n",
    "    ]\n",
    ")\n",
    "output['choices'][0][\"message\"]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5800d",
   "metadata": {},
   "source": [
    "### Llama.cpp'de JSON ve JSON Schema Modları\n",
    "\n",
    "Llama.cpp'nin JSON ve JSON Schema modları, model çıktılarını yapılandırılmış formatta almak için tasarlanmış güçlü özelliklerdir.\n",
    "\n",
    "\n",
    "\n",
    "Cıktıları güzel. Chat olunca daha iyi performasn gösteriyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Yanıtlarınızı JSON formatında verin.\"},\n",
    "        {\"role\": \"user\", \"content\": \"2020 Dünya Serisi'ni kim kazandı?\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},  # JSON modunu aktifleştirir\n",
    "    temperature=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fbcf093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"answer\": \"Los Angeles Dodgers\"\\n}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99210da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 21 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   15802.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   19683.88 ms /    24 runs   (  820.16 ms per token,     1.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   21531.95 ms /    25 tokens\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Yanıtlarınızı belirtilen JSON şemasına uygun verin.\"},\n",
    "        {\"role\": \"user\", \"content\": \"2020 Dünya Serisi'ni kim kazandı?\"}\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"champion\": {\"type\": \"string\"},\n",
    "                \"runner_up\": {\"type\": \"string\"},\n",
    "                \"games_played\": {\"type\": \"integer\"}\n",
    "            },\n",
    "            \"required\": [\"champion\", \"runner_up\"]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5095e14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"champion\": \"Los Angeles Dodgers\", \"runner_up\": \"Tampa Bay Rays\"}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47704f",
   "metadata": {},
   "source": [
    "### Llama.cpp'de Fonksiyon Çağırma (Function Calling) Sistemi\n",
    "\n",
    "Llama.cpp'nin fonksiyon çağırma özelliği, modelin dış işlevleri çağırabilmesini sağlayan güçlü bir mekanizmadır. Bu sistem, OpenAI'nin function calling API'siyle uyumlu şekilde çalışır.\n",
    "\n",
    "- Temel Çalışma Prensibi\n",
    "1) Model, kullanıcı sorgusunu analiz eder\n",
    "\n",
    "2) Tanımlanan fonksiyonlardan uygun olanı seçer\n",
    "\n",
    "3) Fonksiyon parametrelerini otomatik olarak doldurur\n",
    "\n",
    "4) JSON formatında hazırlanmış fonksiyon çağrısını döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be2cf8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./models/gemma-3-12b-it-Q8_0.gguf\",\n",
    "    chat_format=\"chatml-function-calling\", ## Fonksiyon çağırma desteği olan format.functionary: Özel eğitilmiş fonksiyon çağırma modeli\n",
    "    verbose = False,\n",
    "    #n_gpu_layers=40\n",
    "   \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3ee4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"UserDetail\",\n",
    "        \"description\": \"Kullanıcı bilgilerini çıkarır\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"age\": {\"type\": \"integer\"}\n",
    "            },\n",
    "            \"required\": [\"name\", \"age\"]\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4df18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Gerekirse fonksiyonları çağırarak kullanıcıya yardımcı ol.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ali 20 yaşında\"\n",
    "        }\n",
    "    ],\n",
    "    tools=tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"UserDetail\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a170f",
   "metadata": {},
   "source": [
    "### Veri Çıkarımı (Information Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_contact_info\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"phone\": {\"type\": \"string\"},\n",
    "                \"email\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10c8e6",
   "metadata": {},
   "source": [
    "### API Entegrasyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51622c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\"},\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e317cae",
   "metadata": {},
   "source": [
    "### Çoklu Fonksiyon Desteği için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {...}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get stock price information\",\n",
    "            \"parameters\": {...}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78c90c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Aktif mi: True\n",
      "Kullanılan GPU Katmanları: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU Aktif mi: {torch.cuda.is_available()}\")\n",
    "print(f\"Kullanılan GPU Katmanları: {llm.model_params.n_gpu_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c499948",
   "metadata": {},
   "source": [
    "### Embeddings Nedir\n",
    "\n",
    "Embeddings , metinleri sayısal vektörlere dönüştüren yapılardır. Bu vektörler\n",
    "- Kelime veya cümleleri sabit uzunlukta sayı dizilerine çevirir\n",
    "\n",
    "- Anlamsal ilişkileri yakalar (benzer anlamlar, benzer vektörler oluşturur)\n",
    "\n",
    "- Metinler üzerinde matematiksel işlemlere izin verir (örneğin benzer ifadeleri bulma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2905b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "\n",
    "llm = llama_cpp.Llama(model_path=\"./models/gemma-3-12b-it-Q8_0.gguf\", embedding=True)\n",
    "\n",
    "embeddings = llm.create_embedding(\"Hello, world!\")\n",
    "\n",
    "\n",
    "embeddings = llm.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad796ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3eec88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
